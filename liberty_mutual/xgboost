require(xgboost)
require(methods)
library(ggplot2)
library(caret)

# build Gini functions for use in custom xgboost evaluation metric
SumModelGini <- function(solution, submission) {
  df = data.frame(solution = solution, submission = submission)
  df <- df[order(df$submission, decreasing = TRUE),]
  df$random = (1:nrow(df))/nrow(df)
  totalPos <- sum(df$solution)
  df$cumPosFound <- cumsum(df$solution) # this will store the cumulative number of positive examples found (used for computing "Model Lorentz")
  df$Lorentz <- df$cumPosFound / totalPos # this will store the cumulative proportion of positive examples found ("Model Lorentz")
  df$Gini <- df$Lorentz - df$random # will store Lorentz minus random
  return(sum(df$Gini))
}

NormalizedGini <- function(solution, submission) {
  SumModelGini(solution, submission) / SumModelGini(solution, solution)
}

# wrap up into a function to be called within xgboost.train
evalgini <- function(preds, dtrain) {
  labels <- getinfo(dtrain, "label")
  err <- NormalizedGini(as.numeric(labels),as.numeric(preds))
  return(list(metric = "Gini", value = err))
}



##KCIKING OFF SPLITT
y = as.character(train$Hazard)
y = as.character(train_log$Hazard)
y = as.numeric(y)

#enrollment_train_agg <- enrollment_train_agg[,-43]
categoricals_train <- with(train, data.frame(model.matrix(~T1_V4+T1_V5+T1_V6+
              T1_V7+T1_V8+T1_V9+T1_V11+T1_V12+T1_V15+T1_V16+T1_V17+T2_V3+
              T2_V5+T2_V11+T2_V12+T2_V13)))

categoricals_train_log <- with(train_log, data.frame(model.matrix(~T1_V4+T1_V5+T1_V6+
                                                            T1_V7+T1_V8+T1_V9+T1_V11+T1_V12+T1_V15+T1_V16+T1_V17+T2_V3+
                                                            T2_V5+T2_V11+T2_V12+T2_V13)))

categoricals_test <- with(test, data.frame(model.matrix(~T1_V4+T1_V5+T1_V6+
                                                      T1_V7+T1_V8+T1_V9+T1_V11+T1_V12+T1_V15+T1_V16+T1_V17+T2_V3+
                                                      T2_V5+T2_V11+T2_V12+T2_V13)))


vars_to_drop <- c("T1_V4", "T1_V5", "T1_V6",
              "T1_V7", "T1_V8", "T1_V9" ,"T1_V11", "T1_V12",
              "T1_V15", "T1_V16" ,"T1_V17","T2_V3"
              ,"T2_V5", "T2_V11","T2_V12","T2_V13")

train <- train[, !(colnames(train) %in% vars_to_drop)]
train_log <- train_log[, !(colnames(train_log) %in% vars_to_drop)]
test <- test[, !(colnames(test) %in% vars_to_drop)]


train <- cbind(train, categoricals_train)
train_log <- cbind(train_log, categoricals_train_log)
test <- cbind(test, categoricals_test)


x = rbind(train[,3:ncol(train)],test[,2:ncol(test)])
#x= rbind(train_log[,3:ncol(train_log)],test[,2:ncol(test)])

x = as.matrix(x)
x = matrix(as.numeric(x),nrow(x),ncol(x))

trind = 1:length(y)
teind = (nrow(train)+1):nrow(x)

# Set necessary parameter

param <- list("objective" = "reg:linear",
              "eta"= 0.05
              ,"min_child_weight" = 6
              ,"subsample" = 0.7
              ,"colsample_bytree" = 0.7
              ,"scale_pos_weight" = 1
              ,"max_Depth" = 7)


# Run Cross Valication
cv.nround = 2000
bst.cv = xgb.cv(param=param, data = x[trind,], label = y, 
                nfold = 5, nrounds=cv.nround)

# Train the model
nround = 210
bst = xgboost(param=param, data = x[trind,], label = y, nrounds=nround)

# Make prediction
xg_pred_log = as.numeric(predict(bst,x[teind,]))
xg_pred = as.numeric(predict(bst,x[teind,]))
xg_final = xg_pred_log*8.6 + 1.4*xg_pred + 2*predTotal

subm <- data.frame(cbind(Id = test$Id,Hazard = xg_final))
write.table(subm, "submission_xgboost_ensemble.csv",sep=",", row.names=FALSE)

###goes up 0.3804
